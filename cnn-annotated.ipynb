{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "06f6d431-fa9b-4ef6-b483-aecff74b14be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Begin MNIST with CNN demo \n",
      "\n",
      "Creating 1000-item train Dataset from text file \n",
      "\n",
      "Creating CNN network with 2 conv and 3 linear \n",
      "\n",
      "bat_size =  10 \n",
      "loss = CrossEntropyLoss()\n",
      "optimizer = SGD\n",
      "max_epochs =  50 \n",
      "lrn_rate = 0.005 \n",
      "\n",
      "Starting training\n",
      "epoch =    0   loss = 230.0792\n",
      "epoch =    5   loss = 227.4109\n",
      "epoch =   10   loss = 201.8226\n",
      "epoch =   15   loss = 86.4408\n",
      "epoch =   20   loss = 62.5790\n",
      "epoch =   25   loss = 49.5576\n",
      "epoch =   30   loss = 40.5146\n",
      "epoch =   35   loss = 31.5626\n",
      "epoch =   40   loss = 28.5285\n",
      "epoch =   45   loss = 21.5814\n",
      "Done \n",
      "\n",
      "Computing model accuracy\n",
      "Accuracy on training data = 0.9650\n",
      "Accuracy on test data = 0.9600\n",
      "\n",
      "Making prediction for fake image: \n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Predicted class is 'four'\n",
      "\n",
      "Saving trained model state\n",
      "\n",
      "End MNIST CNN demo \n"
     ]
    }
   ],
   "source": [
    "# mnist_cnn.py\n",
    "# PyTorch 1.10.0-CPU Anaconda3-2020.02  Python 3.7.6\n",
    "# Windows 10 \n",
    "\n",
    "# reads MNIST data from text file rather than using\n",
    "# built-in black box Dataset from torchvision\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch as T\n",
    "\n",
    "device = T.device('cpu')\n",
    "\n",
    "# -----------------------------------------------------------\n",
    "\n",
    "class MNIST_Dataset(T.utils.data.Dataset):\n",
    "  # 784 tab-delim pixel values (0-255) then label (0-9)\n",
    "  def __init__(self, src_file):\n",
    "    all_xy = np.loadtxt(src_file, usecols=range(785),\n",
    "      delimiter=\"\\t\", comments=\"#\", dtype=np.float32)\n",
    "\n",
    "    tmp_x = all_xy[:, 0:784]  # all rows, cols [0,783]\n",
    "    tmp_x /= 255\n",
    "    tmp_x = tmp_x.reshape(-1, 1, 28, 28)  # bs, chnls, 28x28\n",
    "    tmp_y = all_xy[:, 784]    # 1-D required\n",
    "\n",
    "    self.x_data = \\\n",
    "      T.tensor(tmp_x, dtype=T.float32).to(device)\n",
    "    self.y_data = \\\n",
    "      T.tensor(tmp_y, dtype=T.int64).to(device) \n",
    "\n",
    "  def __len__(self):\n",
    "    return len(self.x_data)\n",
    "\n",
    "  def __getitem__(self, idx):\n",
    "    lbl = self.y_data[idx]  # no use labels\n",
    "    pixels = self.x_data[idx] \n",
    "    return (pixels, lbl)\n",
    "\n",
    "# -----------------------------------------------------------\n",
    "\n",
    "# definition of the cnn\n",
    "# https://pytorch.org/docs/stable/generated/torch.nn.Module.html\n",
    "class Net(T.nn.Module):\n",
    "  def __init__(self):\n",
    "    super(Net, self).__init__()  # pre Python 3.3 syntax\n",
    "\n",
    "    # https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html\n",
    "    self.conv1 = T.nn.Conv2d(1, 32, 5)  # chnl-in, out, krnl\n",
    "    # note that the output of this layer 32, is the input of the next layer\n",
    "    self.conv2 = T.nn.Conv2d(32, 64, 5)\n",
    "    # https://pytorch.org/docs/stable/generated/torch.nn.Linear.html\n",
    "    # linear layer does the actual classification part\n",
    "    self.fc1 = T.nn.Linear(1024, 512)   # [64*4*4, x] this is the output of the last conv layer\n",
    "    # output of this layer is input of next and so forth and so on\n",
    "    self.fc2 = T.nn.Linear(512, 256)\n",
    "    self.fc3 = T.nn.Linear(256, 10)     # 10 classes\n",
    "    # https://pytorch.org/docs/stable/generated/torch.nn.MaxPool2d.html\n",
    "    self.pool1 = T.nn.MaxPool2d(2, stride=2) # kernel, stride\n",
    "    # applying a kernel, find the max value in the four cells the kernel is applied to\n",
    "    # then stride by 2 (so move the kernel by 2 cells then apply again,\n",
    "    # so it is summarizing the image; by taking the average of the four cells\n",
    "    # you're downsampling the data - keeping most of the integrity but reducing the overall\n",
    "    # data to just its most important elements\n",
    "    self.pool2 = T.nn.MaxPool2d(2, stride=2)\n",
    "    # https://pytorch.org/docs/stable/generated/torch.nn.Dropout.html\n",
    "    self.drop1 = T.nn.Dropout(0.25)\n",
    "    # works the same way as a pooling layer but instead of pooling, we're just\n",
    "    # dropping the percent of the values we give as input viz. 25%\n",
    "    self.drop2 = T.nn.Dropout(0.50)  \n",
    "\n",
    "  # https://www.tutorialexample.com/understand-pytorch-module-forward-function-pytorch-tutorial/\n",
    "  # how you organize the layers\n",
    "  # cnn is basically 2 parts:\n",
    "  #    convolution which extracts the features that are most important \n",
    "  #        to the images you're analyzing\n",
    "  #    then linear layers identify/label those features\n",
    "  def forward(self, x):\n",
    "    # convolution phase         # x is [bs, 1, 28, 28]\n",
    "    z = T.relu(self.conv1(x))   # Size([bs, 32, 24, 24]) calls conv1 then runs through the activation function (relu)\n",
    "    z = self.pool1(z)           # Size([bs, 32, 12, 12]) then pass it through the 1st pooling layer\n",
    "    z = self.drop1(z)                                    # then pass through drop\n",
    "    z = T.relu(self.conv2(z))   # Size([bs, 64, 8, 8]) \n",
    "    z = self.pool2(z)           # Size([bs, 64, 4, 4])\n",
    "   \n",
    "    # by now reduced data size dramatically through convolution\n",
    "    # https://en.wikipedia.org/wiki/Convolution\n",
    "    # neural network phase\n",
    "    z = z.reshape(-1, 1024)     # Size([bs, 1024]) reshape data\n",
    "    z = T.relu(self.fc1(z))     # Size([bs, 512]) call 1st fully connected layer\n",
    "    z = self.drop2(z)\n",
    "    z = T.relu(self.fc2(z))     # Size([bs, 256])\n",
    "    z = self.fc3(z)             # Size([bs, 10])\n",
    "    return z\n",
    "\n",
    "def accuracy(model, ds):\n",
    "  ldr = T.utils.data.DataLoader(ds,\n",
    "    batch_size=len(ds), shuffle=False)\n",
    "  n_correct = 0\n",
    "  for data in ldr:\n",
    "    (pixels, labels) = data\n",
    "    with T.no_grad():\n",
    "      oupts = model(pixels)\n",
    "    (_, predicteds) = T.max(oupts, 1)\n",
    "    n_correct += (predicteds == labels).sum().item()\n",
    "\n",
    "  acc = (n_correct * 1.0) / len(ds)\n",
    "  return acc\n",
    "\n",
    "# all the conttrol logic os om jere\n",
    "def main():\n",
    "  # 0. setup\n",
    "  print(\"\\nBegin MNIST with CNN demo \")\n",
    "  # setting pytorch and numpy random number seeds to make\n",
    "  # results more reproducible\n",
    "  np.random.seed(1)\n",
    "  T.manual_seed(1)\n",
    "\n",
    "  # 1. create Dataset\n",
    "  print(\"\\nCreating 1000-item train Dataset from text file \")\n",
    "  train_file = \"mnist_train_1000.txt\"\n",
    "  train_ds = MNIST_Dataset(train_file)\n",
    "\n",
    "  # settings to load data in chunks for training set\n",
    "  # batch size is a hyperparamter https://deepai.org/machine-learning-glossary-and-terms/hyperparameter\n",
    "  bat_size = 10\n",
    "  train_ldr = T.utils.data.DataLoader(train_ds,\n",
    "    batch_size=bat_size, shuffle=True)\n",
    "\n",
    "  # 2. create network\n",
    "  print(\"\\nCreating CNN network with 2 conv and 3 linear \")\n",
    "  net = Net().to(device)\n",
    "\n",
    "  # 3. train model\n",
    "  max_epochs = 50  # 100 gives better results\n",
    "  ep_log_interval = 5\n",
    "  lrn_rate = 0.005\n",
    "  \n",
    "  loss_func = T.nn.CrossEntropyLoss()  # does log-softmax()\n",
    "  # recall that this is the same thing we did in the other post\n",
    "  # except the opmitizer we use here is SGD (stochastic gradient descent)\n",
    "  # https://www.geeksforgeeks.org/ml-stochastic-gradient-descent-sgd/\n",
    "  # the post mentions what optimizers are better for certain use cases\n",
    "  # but without elaborating why - this would be interesting to explore further\n",
    "  optimizer = T.optim.SGD(net.parameters(), lr=lrn_rate)\n",
    "  \n",
    "  print(\"\\nbat_size = %3d \" % bat_size)\n",
    "  print(\"loss = \" + str(loss_func))\n",
    "  print(\"optimizer = SGD\")\n",
    "  print(\"max_epochs = %3d \" % max_epochs)\n",
    "  print(\"lrn_rate = %0.3f \" % lrn_rate)\n",
    "\n",
    "  print(\"\\nStarting training\")\n",
    "  # https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module.train\n",
    "  net.train()  # set mode\n",
    "  for epoch in range(0, max_epochs):\n",
    "    ep_loss = 0  # for one full epoch\n",
    "    for (batch_idx, batch) in enumerate(train_ldr):\n",
    "      (X, y) = batch  # X = pixels, y = target labels\n",
    "      optimizer.zero_grad()\n",
    "      oupt = net(X)\n",
    "      loss_val = loss_func(oupt, y)  # a tensor\n",
    "      ep_loss += loss_val.item()  # accumulate\n",
    "      loss_val.backward()  # compute gradients\n",
    "      optimizer.step()     # update weights\n",
    "    if epoch % ep_log_interval == 0:\n",
    "      print(\"epoch = %4d   loss = %0.4f\" % (epoch, ep_loss))\n",
    "  print(\"Done \") \n",
    "\n",
    "  # 4. evaluate model accuracy\n",
    "  print(\"\\nComputing model accuracy\")\n",
    "  # https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module.eval\n",
    "  net.eval()\n",
    "  acc_train = accuracy(net, train_ds)  # all at once\n",
    "  print(\"Accuracy on training data = %0.4f\" % acc_train)\n",
    "\n",
    "  test_file = \"mnist_test_100.txt\"\n",
    "  test_ds = MNIST_Dataset(test_file)\n",
    "  net.eval()\n",
    "  acc_test = accuracy(net, test_ds)  # all at once\n",
    "  print(\"Accuracy on test data = %0.4f\" % acc_test)\n",
    "\n",
    "  # 5. make a prediction\n",
    "  print(\"\\nMaking prediction for fake image: \")\n",
    "  # https://numpy.org/doc/stable/reference/generated/numpy.zeros.html\n",
    "  x = np.zeros(shape=(28,28), dtype=np.float32)\n",
    "  # create the following markes in the array\n",
    "  for row in range(5,23):\n",
    "    x[row][9] = 180  # vertical line\n",
    "  for rc in range(9,19):\n",
    "    x[rc][rc] = 250  # diagonal\n",
    "  for col in range(5,15):  \n",
    "    x[14][col] = 200  # horizontal\n",
    "  x /= 255.0\n",
    "\n",
    "  plt.tight_layout()\n",
    "  # normalizes input using the min & max values using gray-reversed colormap\n",
    "  plt.imshow(x, cmap=plt.get_cmap('gray_r'))\n",
    "  plt.show()\n",
    "\n",
    "  # predicting label/digit for fake image\n",
    "  digits = ['zero', 'one', 'two', 'three', 'four', 'five', \n",
    "    'six', 'seven', 'eight', 'nine' ]\n",
    "  x = x.reshape(1, 1, 28, 28)  # 1 image, 1 channel (you'd use 3 channels for color)\n",
    "  # output is a set of 10 logits where the largest value is the predicted class\n",
    "  # logit function is the quantile function associated with the standard logistic distribution\n",
    "  # logistic distribution is a continuous probability distribution. Its cumulative distribution\n",
    "  # function is the logistic function, which appears in logistic regression and feedforward neural networks\n",
    "  x = T.tensor(x, dtype=T.float32).to(device)\n",
    "  with T.no_grad():\n",
    "    # calling net forward on x\n",
    "    oupt = net(x)  # 10 logits like [[-0.12, 1.03, . . ]]\n",
    "  am = T.argmax(oupt) # 0 to 9\n",
    "  # logit values do not sum to 1 so you might want to apply the softmax() function to convert the logits to pseudo-probabilities\n",
    "  print(\"\\nPredicted class is \\'\" + digits[am] + \"\\'\")\n",
    "\n",
    "  # 6. save model\n",
    "  print(\"\\nSaving trained model state\")\n",
    "  fn = \"mnist_model.pt\"\n",
    "  # state_dict() method saves model weights and biases, but does not save the model definition\n",
    "  T.save(net.state_dict(), fn)  \n",
    "\n",
    "  print(\"\\nEnd MNIST CNN demo \")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "  main()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90062d6b-14a8-421a-a376-376e8e6787e5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
