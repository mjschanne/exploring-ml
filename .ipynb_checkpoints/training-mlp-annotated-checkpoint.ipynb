{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7781e176-38aa-4214-a569-48884f938257",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x1b102a4cbb0>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn                         # to load all neural net functionality\n",
    "import torch.nn.functional as F               # adds some efficiency\n",
    "from torch.utils.data import DataLoader       # lets us load data in batches\n",
    "from torchvision import datasets, transforms\n",
    "import sklearn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import confusion_matrix  # for evaluating results\n",
    "import matplotlib.pyplot as plt \n",
    "%matplotlib inline\n",
    "import time\n",
    "torch.manual_seed(101)                        # for consistent results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b92f12ee-ed1e-4e28-bc7d-b53936ce992e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set our datasets, one for training and one for test\n",
    "Transform = transforms.ToTensor()\n",
    "train = datasets.MNIST(root='../DATA', train=True, download=False, transform=Transform)\n",
    "test = datasets.MNIST(root='../DATA', train=False, download=False, transform=Transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "082b880c-8510-4d55-9828-5148b0dbdd0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch the training data into batches of 100, shuffle after each epoch\n",
    "# use smaller batches of 100 to update weights more often - this batch size is a hyperparameter\n",
    "# that can be refined\n",
    "train_loader = DataLoader(train, batch_size=100, shuffle=True)\n",
    "# testing has no gradients or learning taking place so performance is less of a constraint and\n",
    "# batch size can be as large as possible\n",
    "test_loader = DataLoader(test, batch_size=500, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "19c788b3-d6f2-4c56-8259-261fc97fe1a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this perceptron will ingest our data to create a model\n",
    "# it is based on the module class in pytorch\n",
    "class MultilayerPerceptron(nn.Module): \n",
    "    def __init__(self, input_size=784, output_size=10, layers=[120,84]):\n",
    "        super().__init__()\n",
    "        # an __init__() call to the parent class must be made before assignment on the child\n",
    "        # applies a linear transformation in this layer to the incoming data\n",
    "        # https://www.youtube.com/watch?v=4PCktDZJH8E&t=12s\n",
    "        self.d1 = nn.Linear(input_size,layers[0]) # hidden layer 1\n",
    "        # pass the results to the next layer and apply another linear transformation\n",
    "        self.d2 = nn.Linear(layers[0], layers[1]) # hidden layer 2\n",
    "        # pass those results to one more linear layer\n",
    "        self.d3 = nn.Linear(layers[1], output_size) # ouytput layer\n",
    "        \n",
    "    # per the documentation in pytorch, forward should be overridden by all subclasses.\n",
    "    # defines the computation that is performed at every call\n",
    "    def forward(self, X):\n",
    "        # applies the rectified linear unit function element-wise.\n",
    "        # from wikipedia: Rectified Linear Unit) activation function is an activation function defined\n",
    "        # as the positive part of its argument: f(x)=x⁺=max(0,x) where x is the input to a neuron. \n",
    "        # This is also known as a ramp function and is analogous to half-wave rectification in electrical engineering\n",
    "        X = F.relu(self.d1(X))\n",
    "        X = F.relu(self.d2(X))\n",
    "        X = self.d3(X)\n",
    "        # https://pytorch.org/docs/stable/generated/torch.nn.LogSoftmax.html\n",
    "        # https://deepai.org/machine-learning-glossary-and-terms/softmax-layer\n",
    "        return F.log_softmax(X, dim=1)\n",
    "    \n",
    "model = MultilayerPerceptron()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "accf3772-ef95-4520-addb-c31dda99460a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://365datascience.com/tutorials/machine-learning-tutorials/cross-entropy-loss/\n",
    "# measures your model’s performance by transforming its variables into real numbers, thus, evaluating the “loss” that’s associated with them. The higher the difference between the two, the higher the loss.\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "# https://arxiv.org/abs/1412.6980\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0a8b4198-221a-40a5-bad8-5828fca229ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial Batch shape: torch.Size([100, 1, 28, 28])\n",
      "Batch shape after flattening torch.Size([100, 784])\n"
     ]
    }
   ],
   "source": [
    "for images, labels in train_loader:\n",
    "    print('Initial Batch shape:', images.size())\n",
    "    break\n",
    "print('Batch shape after flattening', images.view(100, -1).size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9a6c85b6-a2a8-4945-a5ac-be9367f45e2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  0 batch:  600 [ 60000/60000] Train loss: 0.30252102 Train Accuracy: 92.708%\n",
      "epoch:  1 batch:  600 [ 60000/60000] Train loss: 0.14284366 Train Accuracy: 96.062%\n",
      "epoch:  2 batch:  600 [ 60000/60000] Train loss: 0.10179201 Train Accuracy: 96.763%\n",
      "epoch:  3 batch:  600 [ 60000/60000] Train loss: 0.05633691 Train Accuracy: 97.270%\n",
      "epoch:  4 batch:  600 [ 60000/60000] Train loss: 0.09295914 Train Accuracy: 97.445%\n",
      "epoch:  5 batch:  600 [ 60000/60000] Train loss: 0.04685583 Train Accuracy: 97.518%\n",
      "epoch:  6 batch:  600 [ 60000/60000] Train loss: 0.07953373 Train Accuracy: 97.723%\n",
      "epoch:  7 batch:  600 [ 60000/60000] Train loss: 0.16864368 Train Accuracy: 97.910%\n",
      "epoch:  8 batch:  600 [ 60000/60000] Train loss: 0.12595069 Train Accuracy: 98.080%\n",
      "epoch:  9 batch:  600 [ 60000/60000] Train loss: 0.05573305 Train Accuracy: 98.172%\n"
     ]
    }
   ],
   "source": [
    "epochs = 10\n",
    "train_losses = []\n",
    "test_losses = []\n",
    "train_correct = []\n",
    "test_correct = []\n",
    "\n",
    "for i in range(epochs):\n",
    "    trn_corr = 0\n",
    "    tst_corr = 0\n",
    "    \n",
    "    # Run the training batches\n",
    "    for b, (X_train, y_train) in enumerate(train_loader):\n",
    "        b+=1\n",
    "        \n",
    "        # Apply the model\n",
    "        y_pred = model(X_train.view(100, -1)) # Here we flatten x_train\n",
    "        loss = criterion(y_pred, y_train)\n",
    "    \n",
    "        # Calculate the number of correct predictions\n",
    "        predicted = torch.max(y_pred.data, 1)[1] # the prediction that has the maximum probability\n",
    "        batch_corr = (predicted == y_train).sum()\n",
    "        trn_corr += batch_corr\n",
    "    \n",
    "        # Update parameters\n",
    "        optimizer.zero_grad() # reset the gradients after each training step\n",
    "        loss.backward() # to trigger backprop\n",
    "        optimizer.step() # perform parameter update\n",
    "    \n",
    "        # print interim results\n",
    "        if b%600 == 0:\n",
    "            print(f'epoch: {i:2} batch: {b:4} [{100*b:6}/60000] Train loss: {loss.item():10.8f} Train Accuracy: {trn_corr.item()*100/60000:.3f}%')\n",
    "            \n",
    "        # update training loss and accur\n",
    "        train_losses.append(loss)\n",
    "        train_correct.append(trn_corr)\n",
    "        \n",
    "    # Run the testing batches\n",
    "    with torch.no_grad(): # don't calculate gradients during testing\n",
    "        # https://www.quora.com/In-deep-learning-why-dont-we-use-the-whole-training-set-to-compute-the-gradient\n",
    "        for b, (X_test, y_test) in enumerate(test_loader):\n",
    "            \n",
    "            # Apply the model\n",
    "            y_val = model(X_test.view(500, -1)) # Here we flatten X_test\n",
    "            \n",
    "            # Tally the number of correct predictions\n",
    "            predicted = torch.max(y_val.data, 1)[1]\n",
    "            tst_corr += (predicted == y_test).sum()\n",
    "\n",
    "            # Update test loss & accuracy for the epoch\n",
    "            loss = criterion(y_val, y_test)\n",
    "            test_losses.append(loss)\n",
    "            test_correct.append(tst_corr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b16319a-edf3-4874-8063-7b94c33201ed",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
