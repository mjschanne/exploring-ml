{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a2370ff9-8536-4c6b-853f-150e43f43bc7",
   "metadata": {},
   "source": [
    "# Exploring MNIST Dataset using PyTorch to Train an MLP\n",
    "\n",
    "From the visual search for improved product discoverability to face recognition on social networks- image classification is fueling a visual revolution online and has taken the world by storm. [Image classification](https://www.projectpro.io/article/deep-learning-for-image-classification-in-python-with-cnn/418), a subfield of computer vision helps in processing and classifying objects based on trained algorithms. Image Classification had its Eureka moment back in 2012 when [Alexnet](https://en.wikipedia.org/wiki/AlexNet) won the [ImageNet](https://en.wikipedia.org/wiki/ImageNet#ImageNet_Challenge) challenge and since then there has been an exponential growth in the field. While we humans take our ability to easily classify objects surrounding us because our brains have been trained unconsciously with the same set of images, the problem is not that easy after all. Several factors like view-point variation, size variation, occlusion(blending of objects with other objects in the image), differences in the direction and source of light make it difficult for machines to classify images correctly. Nonetheless, it is an exciting and growing field and there can't be a better way to learn the basics of image classification than to classify images in the MNIST dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b346f06-3565-4506-a589-02e528a23ef3",
   "metadata": {},
   "source": [
    "## What is the MNIST dataset?\n",
    "Before we go any further let's see what is MNIST dataset.\n",
    "\n",
    "![What is MNIST Dataset](https://dezyre.gumlet.io/images/Exploring+MNIST+Dataset+using+PyTorch+to+Train+an+MLP/MNIST+Dataset.png?w=1080&dpr=1.0)\n",
    "\n",
    "MNIST stands for Modified National Institute of Standards and Technology and is a database of 60,000 small square 28x28 pixel grayscale images. MNIST handwritten digits dataset is the most used for learning Image Recognition. It is labeled in the sense that each image of a handwritten digit has the corresponding numeral value attached to it. This helps our Algorithm/Neural Network to learn which image stands for which number (0-9) and to learn hidden patterns in human writing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60cc0d17-c7c3-4742-932b-43a90f44a855",
   "metadata": {},
   "source": [
    "## Types of MNIST Dataset\n",
    "\n",
    "While the handwritten MNIST is the most popular one, there are 6 different extended variations of MNIST:\n",
    "\n",
    "1) [Fashion MNIST](https://github.com/zalandoresearch/fashion-mnist): This dataset from Zalando Research contains images of 10 classes consisting of clothing apparel and accessories like ankle boots, bags, coats, dresses, pullovers, sandals, shirts, sneakers, etc. instead of handwritten digits. The images are grayscale just like the original MNIST.\n",
    "\n",
    "2) [3D MNIST](https://www.kaggle.com/daavoo/3d-mnist): While the original MNIST has 28X28 grayscale (one channel) images, 3D MNIST has images with 3 channels (vis. Red, Green, Blue) like any other color-image out there. It provides a good way to start with 3D Computer Vision Problems.\n",
    "\n",
    "3) [EMNIST](https://www.nist.gov/itl/products-and-services/emnist-dataset): EMNIST is a set of handwritten letters contrary to MNIST which only has handwritten digits. The structure is pretty much the same as MNIST containing grayscale 28X28 images.\n",
    "\n",
    "4) [Sign Language MNIST](https://www.kaggle.com/datamunge/sign-language-mnist):  It is like EMNIST, in the sense that it has images of sign language interpretations of the English alphabets(A-Z). It poses a little more challenging problem of hand gesture recognition and therefore has more useful real-world applications.\n",
    "\n",
    "5) [Colorectal Histology MNIST](https://www.kaggle.com/kmader/colorectal-histology-mnist): The dataset serves a much more interesting MNIST problem for biologists by focusing on histology tiles from patients with colorectal cancer - affecting colon or rectum in the human body. In particular, the data has 8 different classes of cancerous tissue.\n",
    "\n",
    "6) [Skin Cancer MNIST](https://www.kaggle.com/kmader/skin-cancer-mnist-ham10000): It is a medical dataset containing images of skin lesions/cancers along with their corresponding labels. This dataset was made for the 2018 Skin Lesion Detection Challenge. It can be used as a primary dataset for anyone trying to tackle a medical classification problem using deep learning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06b3301b-4713-429b-8e3c-0b24fcab02b4",
   "metadata": {},
   "source": [
    "## MNIST Dataset Download - Steps to Follow\n",
    "\n",
    "Let’s get our hands dirty! While MNIST is also available in the CSV format, for the purpose of this notebook we'll use the original MNIST in ubyte.\n",
    "\n",
    "Follow these simple steps to download and store MNIST on your local machine:\n",
    "1. Go to http://yann.lecun.com/exdb/mnist/ and download all the four files(`train-images-idx3-ubyte.gz`, `train-labels-idx1-ubyte.gz`, `t10k-labels-idx1-ubyte.gz`, `t10k-images-idx3-ubyte.gz`) for train images and labels along with test images and labels.\n",
    "2. In the same directory as your notebook create a folder name `DATA` and inside it creates a folder called `MNIST` and place all these 4 files there.\n",
    "3. Now, leave it to Pytorch to load the files.\n",
    "There are a lot of Deep Learning Frameworks out there that you can use like Keras, Mxnet, Pytorch.\n",
    "- Keras is an open-source framework for building Artificial Neural Networks and it runs on top of TensorFlow (which provides a low-level implementation of NN), thus providing a layer of abstraction and making it easy to use.\n",
    "- Mxnet is also another open-source framework provided by Apache. The main advantage of Mxnet is that it’s scalable and supports multiple programming languages.\n",
    "- We'll be using [Pytorch](https://www.projectpro.io/article/pytorch-vs-tensorflow-2021-a-head-to-head-comparison/416) because the code is more Python-like and the implementation of the Neural Network is not hidden behind layers of abstraction. So basically, coding ends up being more intuitive."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8d28de0-3d4a-48f5-a44a-85e42922bafd",
   "metadata": {},
   "source": [
    "## Import Libraries \n",
    "\n",
    "You can install torch and torchvison from pytorch.org, choose the applicable OS, language, etc.\n",
    "\n",
    "Okay, time to load some libraries we will be needing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "35ae4bcd-2caf-4417-a9c6-bdfa0c6bf7dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x18927daf6f0>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn                         # to load all neural net functionality\n",
    "import torch.nn.functional as F               # adds some efficiency\n",
    "from torch.utils.data import DataLoader       # lets us load data in batches\n",
    "from torchvision import datasets, transforms\n",
    "import sklearn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import confusion_matrix  # for evaluating results\n",
    "import matplotlib.pyplot as plt \n",
    "%matplotlib inline\n",
    "import time\n",
    "torch.manual_seed(101)                        # for consistent results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2a17387-4c3f-49c3-b7d6-816d7fb4d7d3",
   "metadata": {},
   "source": [
    "## [Data Preparation](https://www.projectpro.io/article/why-data-preparation-is-an-important-part-of-data-science/242) MNIST Dataset  \n",
    "\n",
    "Pytorch has a very convenient way to load the MNIST data using datasets.MNIST instead of data structures such as NumPy arrays and lists. Deep learning models use a very similar DS called a Tensor. When compared to arrays tensors are more computationally efficient and can run on GPUs too. We will convert our MNIST images into tensors when loading them. There are lots of other transformations that you can do using torchvision.transforms like Reshaping, normalizing, etc. on your images but we won't need that since MNIST is a very primitive dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6b405ae5-c5e0-4b06-a4a2-91b9610b810f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset MNIST\n",
       "    Number of datapoints: 60000\n",
       "    Root location: ../DATA\n",
       "    Split: Train\n",
       "    StandardTransform\n",
       "Transform: ToTensor()"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Transform = transforms.ToTensor()\n",
    "train = datasets.MNIST(root='../DATA', train=True, download=False, transform=Transform)\n",
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1f635d3c-ca9c-402a-bb41-8b59c89a1199",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset MNIST\n",
       "    Number of datapoints: 10000\n",
       "    Root location: ../DATA\n",
       "    Split: Test\n",
       "    StandardTransform\n",
       "Transform: ToTensor()"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = datasets.MNIST(root='../DATA', train=False, download=False, transform=Transform)\n",
    "test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d43cfb4-5db1-4540-886b-3eb4b1c02e4f",
   "metadata": {},
   "source": [
    "## Visualizing a Batch of Training Data from the MNIST Dataset\n",
    "\n",
    "The train data has 60,000 images and the test has 10,000. Let's look at one.\n",
    "\n",
    "Each image is made up of 28X28 pixels. The 1 in torch.size stands for the number of channels, since it's a grayscale image there's only one channel.\n",
    "\n",
    "Before we go any further, the neural network we will be using is the most basic one. So, let’s have a quick introduction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "19e27ce2-45bb-40c9-bd58-8888f3c97386",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape: torch.Size([1, 28, 28]) \n",
      "Label: 5\n"
     ]
    }
   ],
   "source": [
    "image, label = train[0]\n",
    "print('Shape:', image.shape, '\\nLabel:', label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "72154c27-39a7-44b9-ba42-912dbfa6f8b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x1894e1576a0>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(image.reshape((28,28)), cmap=\"gray\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7968fb6c-ae94-449c-bf3b-fd6335aa4376",
   "metadata": {},
   "source": [
    "## Multilayer Perceptron on MNIST Dataset\n",
    "\n",
    "- A multilayer perceptron has several Dense layers of neurons in it, hence the name multi-layer.\n",
    "- These artificial neurons/perceptrons are the fundamental unit in a neural network, quite analogous to the biological neurons in the human brain. The computation happening in a single neuron can be denoted by the equation. N = Wx + b, where x denotes the input to that neuron and W,b stands for weight and bias respectively. These two values are set at random initially and then keep on updating as the network learns.\n",
    "- Each neuron in a layer is connected to every other neuron in its next layer. In MLPs, data only flows forwards hence they are also sometimes called Feed-Forward Networks.\n",
    "\n",
    "There are 3 basic components:\n",
    "\n",
    "1. _Input Layer_- The input layer would take in the input signal to be processed. In our case, it's a tensor of image pixels.\n",
    "2. _Output Layer_- The output layer does the required task of classification/regression. In our case, it outputs one of the 10 classes for digits 0-9 for a given input image.\n",
    "3. _Hidden Layers_- There is an arbitrary number of hidden layers in between the input and output layer that do all the computations in a Multilayer Perceptron. The number of hidden layers and the number of neurons can be decided to keep in mind the fact that one layer's output is the next layer's input.\n",
    "\n",
    "Now, we know the basics of architecture. To understand the working better let's take the example of our use case- image classification with MNIST.\n",
    "\n",
    "![Multi Layer Perceptron Neural Network Architecture](https://dezyre.gumlet.io/images/Exploring+MNIST+Dataset+using+PyTorch+to+Train+an+MLP/Multilayer+Perceptron+Neural+Network+Architecture.png?w=1080&dpr=1.0)\n",
    "\n",
    "I'll try to break down the process into different steps:\n",
    "1. The pixels in the 28X28 handwritten digit image are flattened to form an array of 784-pixel values. Nothing heavy going on here, just decompressing a 2D array into one dimension.\n",
    "2. The function of the input layer is just to pass-on the input (array of 784 pixels) into the first hidden layer.\n",
    "3. The first hidden layer is where the computations start. It has 120 neurons that are each fed the input array. After calculating the result from the formula stated above, each neuron generates an output that is fed into each neuron of the next layer. Except, there is a little twist here. Instead of just simply passing on the result of Wx+b, an activation is calculated on this result.\n",
    "The activation function is used to clip the output in a definite range like 0-1 or -1 to 1, these ranges can be achieved by Sigmoid and Tanh respectively. The activation function we have used here is ReLu. The main advantage of using the ReLu function is that it does not activate all the neurons at the same time thus making it more computationally efficient than Tanh or Sigmoid.\n",
    "\n",
    "![ReLu Activation Function](https://dezyre.gumlet.io/images/Exploring+MNIST+Dataset+using+PyTorch+to+Train+an+MLP/ReLu+Activation+Function.png?w=1080&dpr=1.0)\n",
    "\n",
    "In short, ReLu clips all the negative values and keeps the positive values just the same.\n",
    "4. The same thing happens in the second hidden layer. It has 84 neurons and takes 120 inputs from the previous layer. The output of this layer is fed into the last layer which is the Output Layer.\n",
    "5. The Output Layer has only 10 neurons for the 10 classes that we have(digits between 0-9). There isn't any activation function in the output layer because we'll apply another function later.\n",
    "6. The Softmax takes the output of the last layer(called logits) which could be any 10 real values and converts it into another 10 real values that sum to 1. Softmax transforms the values between 0 and 1, such that they can be interpreted as probabilities. The maximum value pertains to the class predicted by the classifier. In our case, the value is 0.17 and the class is 5.\n",
    "![Softmax](https://dezyre.gumlet.io/images/Exploring+MNIST+Dataset+using+PyTorch+to+Train+an+MLP/Softmax+Function.png?w=1080&dpr=1.0)\n",
    "\n",
    "The process described above is a single forward pass through the network and instead of just sending one image as input in a pass, a batch of images is fed in a single pass.\n",
    "\n",
    "But how does the network learn?\n",
    "\n",
    "After a single pass through the network, the prediction of the model for that batch of images is compared with the actual labels of those images, and a loss is calculated. Based on the value of this loss, a gradient flow backward through the neural network to update weights(W and b) in each layer. This process is called [Backpropagation](https://karpathy.medium.com/yes-you-should-understand-backprop-e2f06eab496b).\n",
    "\n",
    "In the next iteration, the neural network would do a slightly better job while predicting. This process of forward-pass and backpropagation keeps on repeating as we try to minimize our loss and we the end of our training.\n",
    "\n",
    "Now, that we know most of the things, let's dive right into the code.\n",
    "\n",
    "Loading data into batches\n",
    "- From the 60,000 training records, our images would be sent in batches of 100 through 600 iterations.\n",
    "- For training, setting a smaller batch size will enable the model to update the weights more often and learn better, but there's a caveat here with smaller batch sizes. This is a hyperparameter that could be tuned, I would suggest you try smaller and larger batch sizes than 100 and see the results.\n",
    "- During testing, no learning or flow of gradients takes place. So, you can keep the batch size as big as can fit in your RAM.\n",
    "- Setting shuffle to True means that the dataset will be shuffled after each epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c9a46097-280f-46a3-afa9-9102c1eb92de",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train, batch_size=100, shuffle=True)\n",
    "\n",
    "test_loader = DataLoader(test, batch_size=500, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06d974a7-85e8-41c8-af42-c742edd8026d",
   "metadata": {},
   "source": [
    "## Define Neural Network Architecture- Time to define our Model\n",
    "\n",
    "- The code is straightforward. In Pytorch there isn't any implementation for the input layer, the input is passed directly into the first hidden layer. However, you'll find the InputLayer in the Keras implementation.\n",
    "- The number of neurons in the hidden layers and the number of hidden layers is a parameter that can be played with, to get a better result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "42efde51-5402-4143-bc8a-0764ffefaa19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MultilayerPerceptron(\n",
      "  (d1): Linear(in_features=784, out_features=120, bias=True)\n",
      "  (d2): Linear(in_features=120, out_features=84, bias=True)\n",
      "  (d3): Linear(in_features=84, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class MultilayerPerceptron(nn.Module): \n",
    "    def __init__(self, input_size=784, output_size=10, layers=[120,84]):\n",
    "        super().__init__()\n",
    "        self.d1 = nn.Linear(input_size,layers[0]) # hidden layer 1\n",
    "        self.d2 = nn.Linear(layers[0], layers[1]) # hidden layer 2\n",
    "        self.d3 = nn.Linear(layers[1], output_size) # ouytput layer\n",
    "        \n",
    "    def forward(self, X):\n",
    "        X = F.relu(self.d1(X))\n",
    "        X = F.relu(self.d2(X))\n",
    "        X = self.d3(X)\n",
    "        return F.log_softmax(X, dim=1)\n",
    "    \n",
    "model = MultilayerPerceptron()\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae25c971-0884-462e-810d-a632f162cdac",
   "metadata": {},
   "source": [
    "## Specify the Loss Function and the Optimizer\n",
    "\n",
    "Defining loss function and the optimizer\n",
    "\n",
    "There are a lot of loss functions out there like Binary Cross Entropy, Mean Squared Error, Hinged loss etc. The choice of the loss function depends on the problem at hand and the number of classes. Since we are dealing with a Multi-class classification problem, Pytorch's CrossEntropyLoss is our go-to loss function.\n",
    "\n",
    "Let us talk about the elephant in the room -- **the optimizer**. Remember, I mentioned that during Backpropagation, we update the weights according to the loss throughout the iterations. We basically try to minimize loss as we move ahead through our training. This process is called optimization. Optimizers are algorithms that try to find the optimal way to minimize the loss by navigating the surface of our loss function. We use Adam because it's the best optimizer out there, as proven by different experiments in the scientific community."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2a922ae6-9d7a-4147-8a4e-67b46e63100e",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4fb47c0-f58e-47f7-b38e-3094c5fa19eb",
   "metadata": {},
   "source": [
    "## Train the Neural Network- It's time to train our image classification model!\n",
    "\n",
    "Before I write a plethora of code for training, let me explain a few concepts that'll be used.\n",
    "- **Epoch** - An epoch is a single pass through our full training data(60,000 images). An epoch consists of training steps, which is nothing, but the number of batches passed to the model until all the training data is covered.\n",
    "\n",
    "It could be expressed as number of training steps = number of training records/batch size, which is 600(60000/100) in our case. We'll train the model for 10 epochs- the model will see the full training data exactly 10 times.\n",
    "\n",
    "- **Flattening the image** - Instead of sending the image as a 2D tensor, we flatten it in one-dimension.  \n",
    "\n",
    "The code for training is a few-lines in Keras. As you can see, in Pytorch it's way more because there are wrappers only for very essential stuff and the rest is left to the user to play with. In Pytorch, the user gets a better control over training and it also clears the fundamentals behind model training which is necessary for beginners."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "95ab7b8b-6f17-4086-9497-b04f0b10f25c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial Batch shape: torch.Size([100, 1, 28, 28])\n",
      "Batch shape after flattening torch.Size([100, 784])\n"
     ]
    }
   ],
   "source": [
    "for images, labels in train_loader:\n",
    "    print('Initial Batch shape:', images.size())\n",
    "    break\n",
    "print('Batch shape after flattening', images.view(100, -1).size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "9c0890b4-b9da-4482-9364-254e653ecd10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  0 batch:  600 [ 60000/60000] Train loss: 0.05833448 Train Accuracy: 98.620%\n",
      "epoch:  1 batch:  600 [ 60000/60000] Train loss: 0.12061590 Train Accuracy: 98.533%\n",
      "epoch:  2 batch:  600 [ 60000/60000] Train loss: 0.02283354 Train Accuracy: 98.618%\n",
      "epoch:  3 batch:  600 [ 60000/60000] Train loss: 0.00063294 Train Accuracy: 98.720%\n",
      "epoch:  4 batch:  600 [ 60000/60000] Train loss: 0.02380462 Train Accuracy: 98.730%\n",
      "epoch:  5 batch:  600 [ 60000/60000] Train loss: 0.00098125 Train Accuracy: 98.930%\n",
      "epoch:  6 batch:  600 [ 60000/60000] Train loss: 0.00456336 Train Accuracy: 98.655%\n",
      "epoch:  7 batch:  600 [ 60000/60000] Train loss: 0.12516207 Train Accuracy: 98.832%\n",
      "epoch:  8 batch:  600 [ 60000/60000] Train loss: 0.01074768 Train Accuracy: 98.978%\n",
      "epoch:  9 batch:  600 [ 60000/60000] Train loss: 0.02782904 Train Accuracy: 98.908%\n"
     ]
    }
   ],
   "source": [
    "epochs = 10\n",
    "train_losses = []\n",
    "test_losses = []\n",
    "train_correct = []\n",
    "test_correct = []\n",
    "\n",
    "for i in range(epochs):\n",
    "    trn_corr = 0\n",
    "    tst_corr = 0\n",
    "    \n",
    "    # Run the training batches\n",
    "    for b, (X_train, y_train) in enumerate(train_loader):\n",
    "        b+=1\n",
    "        \n",
    "        # Apply the model\n",
    "        y_pred = model(X_train.view(100, -1)) # Here we flatten x_train\n",
    "        loss = criterion(y_pred, y_train)\n",
    "    \n",
    "        # Calculate the number of correct predictions\n",
    "        predicted = torch.max(y_pred.data, 1)[1] # the prediction that has the maximum probability\n",
    "        batch_corr = (predicted == y_train).sum()\n",
    "        trn_corr += batch_corr\n",
    "    \n",
    "        # Update parameters\n",
    "        optimizer.zero_grad() # reset the gradients after each training step\n",
    "        loss.backward() # to trigger backprop\n",
    "        optimizer.step() # perform parameter update\n",
    "    \n",
    "        # print interim results\n",
    "        if b%600 == 0:\n",
    "            print(f'epoch: {i:2} batch: {b:4} [{100*b:6}/60000] Train loss: {loss.item():10.8f} Train Accuracy: {trn_corr.item()*100/60000:.3f}%')\n",
    "            \n",
    "        # update training loss and accur\n",
    "        train_losses.append(loss)\n",
    "        train_correct.append(trn_corr)\n",
    "        \n",
    "    # Run the testing batches\n",
    "    with torch.no_grad(): # don't calculate gradients during testing\n",
    "        for b, (X_test, y_test) in enumerate(test_loader):\n",
    "            \n",
    "            # Apply the model\n",
    "            y_val = model(X_test.view(500, -1)) # Here we flatten X_test\n",
    "            \n",
    "            # Tally the number of correct predictions\n",
    "            predicted = torch.max(y_val.data, 1)[1]\n",
    "            tst_corr += (predicted == y_test).sum()\n",
    "\n",
    "            # Update test loss & accuracy for the epoch\n",
    "            loss = criterion(y_val, y_test)\n",
    "            test_losses.append(loss)\n",
    "            test_correct.append(tst_corr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f027c9b-5d48-4d82-a338-dfe5474337f6",
   "metadata": {},
   "source": [
    "## Test the Trained Neural Network\n",
    "\n",
    "The training loss keeps on decreasing throughout the epochs and we can conclude that our model is definitely learning. But to gauge the performance of our model we'll have to see how well it does on unseen(test) data.\n",
    "\n",
    "Predictions are made on our test data after training completes in every epoch. Since our model continually keeps getting better, the test accuracy of the last epoch is the best."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "e8007521-fc92-465c-9615-919bac1a0306",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 97.160%\n"
     ]
    }
   ],
   "source": [
    "print(f'Test Accuracy: {test_correct[-1].item()*100/10000:.3f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48f16567-528e-4fcc-9c09-d2781de8b50a",
   "metadata": {},
   "source": [
    "## Visualizing the Test Results\n",
    "\n",
    "It will be intuitive and fun to see the progression of loss and accuracy through the epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "19e0758a-fd4b-4ef1-9bd3-bdb2b936f6ae",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'plt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [1]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mplt\u001b[49m\u001b[38;5;241m.\u001b[39msubplot(\u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m      2\u001b[0m plt\u001b[38;5;241m.\u001b[39mplot(train_losses, label\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtraining loss\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      3\u001b[0m plt\u001b[38;5;241m.\u001b[39mplot(test_losses, label\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvalidation loss\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'plt' is not defined"
     ]
    }
   ],
   "source": [
    "plt.subplot(3, 1, 1)\n",
    "plt.plot(train_losses, label='training loss')\n",
    "plt.plot(test_losses, label='validation loss')\n",
    "plt.title('Loss at the end of each epoch')\n",
    "\n",
    "plt.subplot(3, 1, 3)\n",
    "plt.plot([t/600 for t in train_correct], label='training accuracy')\n",
    "plt.plot([t/100 for t in test_correct], label='validation accuracy')\n",
    "plt.title('Accuracy at the end of each epoch')\n",
    "\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a98ea3b4-7606-45ef-8545-aa95a47b6ca5",
   "metadata": {},
   "source": [
    "## Ending Notes\n",
    "\n",
    "We are at the end and have successfully trained an image recognition model on MNIST dataset.\n",
    "\n",
    "There are several tricks you can try to improve the performance of the model like:\n",
    "- Changing the learning rate in the optimizer.\n",
    "- Decreasing/Increasing the batch size for training.\n",
    "- Changing the number of Neurons in the hidden layers.\n",
    "- Changing the number of hidden layers, while remembering that a layer’s output is the subsequent layer’s input.\n",
    "- You can also try using another optimizer instead of Adam like RMSProp, Adagrad, etc.\n",
    "Handwriting recognition from images isn't only limited to MNIST or understanding the basics of Deep Learning - there is a whole field based around it called [OCR or Optical Character Recognition](https://searchcontentmanagement.techtarget.com/definition/OCR-optical-character-recognition#:~:text=OCR%20(optical%20character%20recognition)%20is,as%20a%20scanned%20paper%20document.). OCR is very useful in digitalizing handwritten documents and is also used by Google Lens to extract text from images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d454e1b7-3d42-47fa-9428-c0adfad2a0d7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
